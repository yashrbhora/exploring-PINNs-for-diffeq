<!doctype html>
<html lang="en">
<head>
<title>Exploring Physics-informed Neural Networks For Differential Equations</title>
<meta property="og:title" content=Your Project Name" />
<meta name="twitter:title" content="Your Project Name" />
<meta name="description" content="Your project about your cool topic described right here." />
<meta property="og:description" content="Your project about your cool topic described right here." />
<meta name="twitter:description" content="Your project about your cool topic described right here." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">Exploring Physics-informed Neural Networks For Differential Equations</nobr>
 <nobr class="widenobr">For DS 4440</nobr>
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>An Analysis of Physics-informed Neural Networks for Differential Equations</h2>
</div>
</div>
<div class="row">
<div class="col">


<h2>Introduction</h2>

<p>The Susceptible-Infectious-Recovered (SIR) model is an epidemiological model used to evaluate and predict the spread
  of infectious diseases within a population. The model consists of three ordinary differential equations:
</p>

<p>The Susceptible Equation: $$\frac{dS}{dt} = \frac{-\beta S(t) I(t)}{{N}}$$</p>

<p>The Infected Equation: $$\frac{dI}{dt} = \frac{\beta S(t) I(t)}{N} - \gamma I(t)$$</p>

<p>The Recovered Equation: $$\frac{dR}{dt} = \gamma I(t)$$</p>

<p>This project aims to apply physics-informed neural networks (PINNs) to the SIR equations. By introducing mathematical
  constraints from the differential equations in the loss function, the neural network is able to better generalize to
  the behavior of the system. The accurate prediction of the SIR model can offer insights into the duration of an
  outbreak, the total number of infections, and the peak of an outbreak. This information is vital for healthcare
  officials to determine the appropriate allocation of resources during a crisis, which can ultimately save lives.
</p>


<h2>Paper Review</h2>
<p>Physics-informed Neural Networks by Raussi et al. presents a method of using neural networks to solve
  differential equations by introducing the mathematical constraints as a regularization agent in the loss function.
  This prior information serves to shrink the solution space of the network, enabling it to quickly learn a generalized
  function that obeys the given laws. Generalization is of increased importance in the context of complex biological and
  physical domains, which are often plagued by low amounts of training data. The authors demonstrate the method by
  applying PINNs to notable problems such as Schrodingers equation and the Allen-Cahn equation. In these cases, PINNs
  are able to generate accurate predictions of the non-linear behavior despite the small data sets. The level of
  performance in these predictions is especially impressive when considering the relative simplicity in the
  implementation of PINNs.
</p>

<p>Though, as the authors recognize, PINNs are not a comprehensive replacement to classical methods. Rather, they offer
  alternative paths to solving partial differential equations with new features such as parameterization and transfer
  learning, which can outweigh analytical techniques in specific domains. In our case, we choose to apply PINNs to the
  SIR model to predict the dynamics of disease spread within a population. PINNs can offer better generalization if
  the epidemiological data has considerable noise or is sparse. The flexibility of PINNs also allows it to be used in
  inverse problems, such as estimating model parameters.
</p>

<h2>Technical Structure</h2>

    <h3>Overview</h3>
    <p>The project employs Physics-Informed Neural Networks (PINNs) to solve the Susceptible-Infectious-Recovered (SIR) model, a set of ordinary differential equations used in epidemiology to predict the spread of infectious diseases. PINNs integrate the mathematical structure of the differential equations directly into the loss function of a neural network. This approach allows the network to leverage both the available data and the underlying physics of the problem, leading to more accurate predictions even with limited or noisy data.</p>

    <h3>Neural Network Architecture</h3>
    <p>The core of our implementation is a neural network designed to approximate the solutions of the SIR model. The network, implemented using PyTorch, consists of five fully connected layers:</p>
    <ul>
        <li>The first layer takes a single input, representing time <i>t</i>, and maps it to 100 neurons.</li>
        <li>The subsequent three layers are hidden layers with 100, 50, and 20 neurons, respectively, each followed by a ReLU activation function to introduce non-linearity.</li>
        <li>The final layer outputs three values corresponding to the susceptible (<i>S</i>), infected (<i>I</i>), and recovered (<i>R</i>) compartments of the SIR model.</li>
    </ul>
    <p>Additionally, the model includes parameters <i>β</i> and <i>γ</i>, representing the infection rate and recovery rate, respectively. These are treated as learnable parameters within the network, allowing the model to adjust these values based on the data.</p>

    <h3>Physics-Informed Loss</h3>
    <p>The key innovation of our approach is the incorporation of the SIR model's differential equations into the loss function. This is achieved through a custom physics-informed loss function that penalizes deviations from the expected dynamics as described by the equations:</p>
    <ul>
        <li>The loss function calculates the derivatives of <i>S</i>, <i>I</i>, and <i>R</i> with respect to time using automatic differentiation.</li>
        <li>It then computes the discrepancies between these derivatives and the expressions given by the SIR model's differential equations.</li>
        <li>Additional terms in the loss function enforce the conservation of the total population and encourage the learnable parameters <i>β</i> and <i>γ</i> to stay within reasonable bounds.</li>
    </ul>
    <p>This physics-informed loss is combined with a traditional mean squared error loss calculated from the training data, allowing the network to learn from both the data and the physics of the problem.</p>

    <h3>Training and Evaluation</h3>
    <p>The model is trained using the Adam optimizer, with a learning rate scheduler to reduce the learning rate when the validation loss plateaus. The training process involves:</p>
    <ol>
        <li>Minimizing the combined physics-informed and data-driven loss.</li>
        <li>Regularly evaluating the model on a validation set to monitor its performance and adjust the learning rate as necessary.</li>
        <li>Saving the model parameters that achieve the lowest validation loss, ensuring that the best-performing model is retained.</li>
    </ol>
    <p>After training, the model's predictions for the SIR curves are compared against the actual data to assess its accuracy and generalization capability.</p>

    <a href="https://github.com/yashrbhora/exploring-PINNs-for-diffeq/blob/main/sir_basic_PINN.ipynb">Click here</a> to view the notebook on GitHub.</p>
 
<h2>Results</h2>

<p>First, we applied PINNs to a simple differential equation: Newton's Cooling Law. This equation is described by:
  $$\frac{dT}{dt} = R (T_{env} - T)$$ where R is the cooling rate, T_env is the temparature of the environment, and T is the temperature of the object. </p>

<div class="twofigures">
  <img src="naive_newton.png" alt="naive newton">
  <img src="pinn_newton.png" alt="pinn newton">
</div>

<p>Next, we trained a simple neural network to model the SIR equations. This network has no prior knowledge of the
  dynamics of the system and is purely minimizing the mean square error loss from the training data. As shown below,
  this network fails to generalize well to the future behavior of the SIR model.
</p>

<img src="naive_SIR.png" alt="naive SIR">

<p>Then, we used the same neural network architecture, but incorportated the physics-informed loss in the optimization.
  As shown below, the nerual network with the same architecture peforms much better in its predictions of the SIR model.
  This demonstrates the value in providing the prior mathematical knowledge from the system of differential equations.
</p>

<img src="pinn_SIR.png" alt="pinn SIR">


<h2>Conclusion</h2>

<p>As shown above, the application of PINNs to the SIR equations generates substantial improvements over basic neural
  network architectures. By informing the loss function with prior information about the system, the model is able to
  better generalize to the long-term behavior of the disease spread. The increased accuracy in modeling these equations
  can provide policymakers with valuable knowledge about the future state of the population. In the future, this work
  could be further progressed by applying PINNs to more complicated epidemiological equations such as the SEIR or SEIRS
  models.
</p>


<h3>References</h3>

<p><a name="raissi-2019">[1]</a> <a href="https://www.sciencedirect.com/science/article/pii/S0021999118307125"
  >M. Raissi, P. Perdikaris, and G.E. Karniadakis.
  <em>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving
    nonlinear partial differential equations.</em></a>
  Journal of Computational Physics, vol. 378, pp. 686-707, February 2019.
</p>


<h2>Team Members</h2>
                                                   
<p>Yash Bhora and Henry Noyes</p>

  
</div><!--col-->
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://ds4440.baulab.info/">About DS 4440</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
